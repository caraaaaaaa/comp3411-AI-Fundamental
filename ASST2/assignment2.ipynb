{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# COMP3411/9418 21T0 Assignment 2\n",
    "\n",
    "- Lecturer: Anna Trofimova\n",
    "- School of Computer Science and Engineering, UNSW Sydney\n",
    "- Last Update 26th January at 07:00am, 2021\n",
    "$$\n",
    "% macros\n",
    "\\newcommand{\\indep}{\\perp \\!\\!\\!\\perp}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student:\n",
    "Zixuan Guo, z5173593"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "This interactive notebook contains the instructions to complete assignment 2; You should submit this notebook with the code and answers in one single file in .ipybn format with the name assignment2.ipybn. **Write your name and zID in the cell above** (to edit the markdown text double-click the cell).\n",
    "\n",
    "There is a maximum file size cap of 5MB, so make sure your submission does not exceed this size. The submitted notebook should contain all your source code and answers. You can add new cells and use markdown text to organise and explain your implementation/answer.\n",
    "\n",
    "Submit your files using give. On a CSE Linux machine, type the following on the command-line:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ give cs3411 ass2 assignment2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission deadline is **3rd February at 11.59pm, 2021.** This is a hard deadline, no extentions will be granted (it is not our decision, it is the reality of the summer term courses)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Late Submission Policy \n",
    "The penalty is set at 20% per late day. This is a ceiling penalty, so if your submission is marked 12/20 and it was submitted two days late, you still get 12/20. If you submit 5 days later, then the penalty is 100% and your mark will be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plagiarism\n",
    "This is an individual assignment. Remember that **all** work submitted for this assignment must be your own **individual** work and no code\n",
    "sharing or copying is allowed. You may use code from the Internet only with suitable attribution\n",
    "of the source in your program. **Do not use public code repositories as your code might be copied.** Keep in mind that sharing parts of assignment solutions is a form of plagiarism. All submitted assignments will be run through plagiarism detection software to detect similarities to other submissions. You should carefully read the UNSW policy on academic integrity and plagiarism. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "Imagine that you have been hired as a Data Scientist by Amazon, and your first task is to evaluate customer sentiment towards their products. Many online stores selling Amazon products provide their customers with an option to leave a review, but they might not have a rating system, or the customers might choose to leave a review without rating. However, you still want to use these reviews in your report. Thus, you need to develop a reliable model that can automatically assign setiment given a review.\n",
    "\n",
    "To develop your model, you have been given a collection of customer reviews on products like Alexa Echo, Echo dots, Alexa Firesticks etc. with their corresponding ratings. The ratings vary from 1 to 5, but to simplify the problem, you will consider reviews with ratings 1 & 2 to have negative sentiment, with 3 having neutral sentiment,  and 4 & 5 having positive sentiment.\n",
    "* Negative: 1 & 2\n",
    "* Neutral: 3\n",
    "* Positive: 4 & 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "When working with a Jupyter notebook, you can edit the \\*.py files either in the Jupyter interface (in your browser) or with your favorite editor (e.g., PyCharm). Whenever you save a \\*.py file, the notebook will reload their content directly.\n",
    "\n",
    "**Do not create new markdown cells, if you want to comment on something then use Raw NBConvert cells.**\n",
    "\n",
    "Below are the libraries that you can use (and need) in this assignment. If you want to use a library that is not in the list then send us an email to confirm (use course email). \n",
    "\n",
    "Run the code below to import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import graphviz\n",
    "from graphviz import Source\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import re\n",
    "import copy\n",
    "\n",
    "\n",
    "#  If you experience problems with downloading stopwords, uncomment and run the code below.\n",
    "#  It will launch NLTK Downloader application so you can download stopwords corpora manually.\n",
    "\n",
    "#import nltk\n",
    "#import ssl\n",
    "#\n",
    "#try:\n",
    "#    _create_unverified_https_context = ssl._create_unverified_context\n",
    "#except AttributeError:\n",
    "#    pass\n",
    "#else:\n",
    "#    ssl._create_default_https_context = _create_unverified_https_context\n",
    "#\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Data [2 marks]\n",
    "\n",
    "In this part of the assignment you need to import the Amazon reviews dataset and analyse its main properties.\n",
    "\n",
    "#### Task 1.1\n",
    "Import the dataset from *amazon_alexa.tsv* file, save it into the variable *data* and change the rating labels as follow: \n",
    " {1: 'negative', 2: 'negative', 3: 'neutral', 4: 'positive', 5: 'positive'}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "\n",
    "data = pd.read_csv('amazon_alexa.tsv', sep='\\t')\n",
    "data.loc[data['rating'] == 1, 'rating'] = 'negative'\n",
    "data.loc[data['rating'] == 2, 'rating'] = 'negative'\n",
    "data.loc[data['rating'] == 3, 'rating'] = 'neutral'\n",
    "data.loc[data['rating'] == 4, 'rating'] = 'positive'\n",
    "data.loc[data['rating'] == 5, 'rating'] = 'positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to plot the data distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['rating'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give a simple description of the class frequency distribution across the full dataset. What do you notice about the distribution? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# It shows a asymmetric distribution, with positive class has the distinct high frequency. Frequency of negative class slightly more than Neutral class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.2\n",
    "\n",
    "To reduce computation time in this task, your will use only the 1000 most frequent tokens from the vocabulary as attributes. Run the code below to convert the input text samples into a document-term matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>!!</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>&amp;#34;Alexa,</th>\n",
       "      <th>&amp;#34;Things</th>\n",
       "      <th>(which</th>\n",
       "      <th>,</th>\n",
       "      <th>-</th>\n",
       "      <th>--</th>\n",
       "      <th>.</th>\n",
       "      <th>...</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>wouldn't</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yet</th>\n",
       "      <th>yet.</th>\n",
       "      <th>you</th>\n",
       "      <th>you're</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   !  !!  &  &#34;Alexa,  &#34;Things  (which  ,  -  --  .  ...  worth  would  \\\n",
       "0  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "1  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "2  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "3  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "4  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "\n",
       "   wouldn't  year  years  yet  yet.  you  you're  your  \n",
       "0         0     0      0    0     0    0       0     0  \n",
       "1         0     0      0    0     0    0       0     0  \n",
       "2         0     0      0    0     0    2       0     0  \n",
       "3         0     0      0    0     0    0       0     0  \n",
       "4         0     0      0    0     0    0       0     0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer=lambda x: x.split(), max_features=1000)\n",
    "\n",
    "x_dense = vectorizer.fit_transform(data['verified_reviews'])\n",
    "x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "x_sparse.head()\n",
    "# print(x_dense)\n",
    "#  (0, 96)\t1\n",
    "#  (0, 621)\t1\n",
    "#  (1, 98)\t1\n",
    "#  (1, 528)\t1\n",
    "#  (2, 129)\t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was written in the helper notebook that the original input representation is bad. Explain why a document-term matrix is a more suitable representation for machine learning tasks with text data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# A document-term matrix  split sentences into words and count their frequency of occurrence, and make into matrix form. \n",
    "If not, machine need to learn every reviews, the problem is that differernt people may have similar meaning but with different expression. Without transform to words matrix, it will be a huge workload for machine learning and make prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two variables *x_dense* and *x_sparce* both represent input data in a document-term form. Explain which one is preferable and why."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Dense matrix uses smaller space than sparse matrix because it does not include data with zero frequency, but data in sparse matrix form is more convenient for machine learning, because we need to use those data for machine learning, data in sparce matrix form makes mechine learning doing basic mathematical operation and other computation easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Decision Tree [2 marks]\n",
    "\n",
    "In this part you need to evaluate how classifier hyperparameters affect their performance. Consider a Decision Tree classifier that splits attributes based on the lowest entropy. For performance evaluation, assume that only the top 1000 most frequent tokens are used as attributes. The first 60% of samples should constitute the traing set while the remaining 40% of samples should constitute the test set.\n",
    "\n",
    "#### Task 2.1\n",
    "In the cell below place your code that computes performance measures for decision trees with different depth limits. <br> \n",
    "Consider the following cases: max_depth = 5, 10, 100, 200, None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5a3aee36b800>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# your code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "y = data['rating']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_sparse, y, test_size=0.4, shuffle = False)\n",
    "\n",
    "\n",
    "for depth in [5, 10, 100, 200, None]:\n",
    "    print('***depth: ', depth)\n",
    "    print()\n",
    "    \n",
    "    # create a decision tree classifier\n",
    "    classifier = tree.DecisionTreeClassifier(max_depth = depth)\n",
    "\n",
    "    # train the model \n",
    "    model = classifier.fit(x_train, y_train)\n",
    "\n",
    "    y_test_predicted = model.predict(x_test)\n",
    "    \n",
    "    # print different evaluation metrics \n",
    "    print(classification_report(y_test, y_test_predicted))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2.2\n",
    "In the cell below explain any difference in performance, and comment on metrics in relation to the depth limit."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# By increasing the depth, the accuracy of predicting slightly increased, especially on negative class and neutral class. The accuracy of predict positive class is higher than negative class and neutral class, one reason is positive class has a big amount of data, after training it can be predicted better than the others. \n",
    "But for 'max_depth = None' which means here's no limit on the max depth, the accuracy is not increased as much, so increase max depth will not "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Pre-processing [3 marks]\n",
    "\n",
    "In this part, you need to evaluate the effect that the pre-processing of input features has on the performance of three classifiers - Decision Tree (DT), Multinomial Naive Bayes (MNB), and  Artificial Neural Networks (ANN). For a Decision Tree classifier, use the parameters from Part2 but do not limit the depth. For a Artificial Neural Network classifier, set the number of hidden neurons to 200.\n",
    "\n",
    "For the performance evaluation, assume that the first 60% of samples constitutes the traing set while the remaining 40% of samples constitutes the test set. All tokens in the vocabulary should be used as attributes.\n",
    "Consider the following scenarios:\n",
    "1. No data pre-processing\n",
    "2. Removal of URLs and tokens that are not made of strings of letters, numbers, or symbols {’, #, @, $} delimited by spaces.\n",
    "3. Apply (2) and make all tokens lowercase.\n",
    "4. Apply (2), (3) and remove all stop words.\n",
    "\n",
    "#### Task 3.1\n",
    "In the cell below place your code that reflects the four scenarios of feature pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# case 1\n",
    "def preprocessing_1(x):\n",
    "    pre1 = x.copy()\n",
    "    return pre1\n",
    "\n",
    "# case 2\n",
    "def preprocessing_2(x):\n",
    "    pre2 = x.copy()\n",
    "    pre2['verified_reviews'] = pre2['verified_reviews'].map(lambda pre: re.sub(r'https?\\S+', '', pre))\n",
    "    pre2['verified_reviews'] = pre2['verified_reviews'].map(lambda pre: re.sub(r'[^A-Za-z\\d\\'\\#\\@\\$\\s*]', '', pre))\n",
    "    return pre2\n",
    "\n",
    "# case 3\n",
    "def preprocessing_3(x):\n",
    "    pre3 = x.copy()\n",
    "    pre3['verified_reviews'] = pre3['verified_reviews'].map(lambda pre: re.sub(r'https?\\S+', '', pre).lower())\n",
    "    pre3['verified_reviews'] = pre3['verified_reviews'].map(lambda pre: re.sub(r'[^A-Za-z\\d\\'\\#\\@\\$\\s*]', '', pre).lower())\n",
    "    return pre3\n",
    "\n",
    "# case 4\n",
    "def preprocessing_4(x):\n",
    "    pre4 = x.copy()\n",
    "    pre4['verified_reviews'] = pre4['verified_reviews'].map(lambda pre: re.sub(r'https?\\S+', '', pre).lower())\n",
    "    pre4['verified_reviews'] = pre4['verified_reviews'].map(lambda pre: re.sub(r'[^A-Za-z\\d\\'\\#\\@\\$\\s*]', '', pre).lower())\n",
    "    \n",
    "    stopword = stopwords.words('english')\n",
    "    listnew = []\n",
    "    for i in pre4.index:\n",
    "        verifiedreviews = pre4.verified_reviews[i].split()\n",
    "        listnew.append(' '.join([item for item in verifiedreviews if item not in stopword]))\n",
    "\n",
    "    pre4['verified_reviews'] = listnew\n",
    "    return pre4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.2 \n",
    "\n",
    "In the cell below, place your code that trains and tests all three classifiers. Pre-process your data according on scenario #4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Decision Tree classifier\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.33      0.41      0.37       108\n",
      "     neutral       0.20      0.18      0.19        67\n",
      "    positive       0.91      0.90      0.91      1085\n",
      "\n",
      "    accuracy                           0.82      1260\n",
      "   macro avg       0.48      0.50      0.49      1260\n",
      "weighted avg       0.83      0.82      0.82      1260\n",
      "\n",
      "\n",
      "2. Multinomial Naive Bayes classifier\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.32      0.43       108\n",
      "     neutral       0.20      0.18      0.19        67\n",
      "    positive       0.91      0.96      0.93      1085\n",
      "\n",
      "    accuracy                           0.86      1260\n",
      "   macro avg       0.58      0.49      0.52      1260\n",
      "weighted avg       0.84      0.86      0.85      1260\n",
      "\n",
      "\n",
      "3. Artificial Neural Networks classifier\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.35      0.24      0.29       108\n",
      "     neutral       0.20      0.10      0.14        67\n",
      "    positive       0.90      0.95      0.93      1085\n",
      "\n",
      "    accuracy                           0.85      1260\n",
      "   macro avg       0.48      0.43      0.45      1260\n",
      "weighted avg       0.82      0.85      0.83      1260\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code   \n",
    "preprocess = preprocessing_4(data)\n",
    "y = data['rating']\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x.split(), max_features=1000)\n",
    "\n",
    "x_dense = vectorizer.fit_transform(preprocess['verified_reviews'])\n",
    "x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_sparse, y, test_size=0.4, shuffle = False)\n",
    "\n",
    "\n",
    "print('1. Decision Tree classifier')\n",
    "print()\n",
    "\n",
    "# create a decision tree classifier\n",
    "classifier = tree.DecisionTreeClassifier()\n",
    "\n",
    "# train the model \n",
    "model = classifier.fit(x_train, y_train)\n",
    "\n",
    "y_test_predicted = model.predict(x_test)\n",
    "\n",
    "# print different evaluation metrics \n",
    "print(classification_report(y_test, y_test_predicted))\n",
    "print()\n",
    "\n",
    "print('2. Multinomial Naive Bayes classifier')\n",
    "print()\n",
    "\n",
    "# create a Multinomial classifier\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# train the model \n",
    "model = classifier.fit(x_train, y_train)\n",
    "\n",
    "y_test_predicted = model.predict(x_test)\n",
    "\n",
    "# print different evaluation metrics \n",
    "print(classification_report(y_test, y_test_predicted))\n",
    "print()\n",
    "\n",
    "print('3. Artificial Neural Networks classifier')\n",
    "print()\n",
    "\n",
    "# create a ANN classifier\n",
    "classifier = MLPClassifier(hidden_layer_sizes=200)\n",
    "\n",
    "# train the model \n",
    "model = classifier.fit(x_train, y_train)\n",
    "\n",
    "y_test_predicted = model.predict(x_test)\n",
    "\n",
    "# print different evaluation metrics \n",
    "print(classification_report(y_test, y_test_predicted))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.3\n",
    "\n",
    "Evaluate the effect each pre-processing scenario has on the performance for the three models. In each case. compute the performance metrics. Write your answer in the cell below. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# By removing more symble and stop words (from scenario 1 to scenario 4) leads to a difference for Decision Tree classifier and Artificial Neural Networks classifier, there's no big differerce for MNB classifier.\n",
    "ANN classifier shows a highest accuracy for the three models in all scenarios. \n",
    "Scenario 4 should give a highest accuracy than the other 3 scenarios for all models, beacuse not only symbles removed, but also removing stopwords. But in my testing accuracy for scenario 4 is lower than 3 for 0.01 for all three classifiers, which in my opinion is a training error for some reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 [5 marks]\n",
    "\n",
    "In this part, you need to evaluate how the number of features and classes affects the performance of the three classifiers - Decision Tree (DT), Multinomial Naive Bayes (MNB), and Artificial Neural Networks (ANN). For a Decision Tree classifier, use the parameters from Part 2 but do not limit the depth. For a Artificial Neural Network classifier set the number of hidden neurons to 200.\n",
    "\n",
    "For the performance evaluation, assume that the first 60% of samples constitute the traing set while the remaining 40% of samples constitute the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.1\n",
    "\n",
    "Consider the following scenarios for attributes:\n",
    "\n",
    "1. Attributes are all tokens in the vocabulary\n",
    "2. Attributes are the top 1000 most frequent tokens in the vocabulary\n",
    "3. Attributes are the top 100 most frequent tokens in the vocabulary\n",
    "4. Attributes are the top 10 most frequent tokens in the vocabulary\n",
    "\n",
    "In the cell below, place your code that returns a document-term representation given the number of attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "def limit_attributes(x, n_attributes):\n",
    "    vectorizer = CountVectorizer(analyzer=lambda x: x.split(), max_features=n_attributes)\n",
    "\n",
    "    x_dense = vectorizer.fit_transform(x['verified_reviews'])\n",
    "    x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "    return x_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.2 \n",
    "\n",
    "In the cell below, place your code that trains and tests all three classifiers. Limit the number of attributes according on scenario #4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***attributes: ALL\n",
      "1. Decision Tree classifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.27      0.28      0.28       108\n",
      "     neutral       0.04      0.03      0.03        67\n",
      "    positive       0.89      0.91      0.90      1085\n",
      "\n",
      "    accuracy                           0.81      1260\n",
      "   macro avg       0.40      0.40      0.40      1260\n",
      "weighted avg       0.80      0.81      0.80      1260\n",
      "\n",
      "\n",
      "2. Multinomial Naive Bayes classifier\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.39      0.46       108\n",
      "     neutral       0.10      0.07      0.09        67\n",
      "    positive       0.91      0.95      0.93      1085\n",
      "\n",
      "    accuracy                           0.85      1260\n",
      "   macro avg       0.53      0.47      0.49      1260\n",
      "weighted avg       0.83      0.85      0.84      1260\n",
      "\n",
      "\n",
      "3. Artificial Neural Networks classifier\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.40      0.46       108\n",
      "     neutral       0.16      0.06      0.09        67\n",
      "    positive       0.90      0.96      0.93      1085\n",
      "\n",
      "    accuracy                           0.87      1260\n",
      "   macro avg       0.53      0.47      0.49      1260\n",
      "weighted avg       0.83      0.87      0.85      1260\n",
      "\n",
      "\n",
      "***attributes:  1000\n",
      "1. Decision Tree classifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.28      0.31      0.29       108\n",
      "     neutral       0.09      0.07      0.08        67\n",
      "    positive       0.90      0.90      0.90      1085\n",
      "\n",
      "    accuracy                           0.81      1260\n",
      "   macro avg       0.42      0.43      0.43      1260\n",
      "weighted avg       0.81      0.81      0.81      1260\n",
      "\n",
      "\n",
      "2. Multinomial Naive Bayes classifier\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.39      0.46       108\n",
      "     neutral       0.10      0.07      0.09        67\n",
      "    positive       0.91      0.95      0.93      1085\n",
      "\n",
      "    accuracy                           0.85      1260\n",
      "   macro avg       0.53      0.47      0.49      1260\n",
      "weighted avg       0.83      0.85      0.84      1260\n",
      "\n",
      "\n",
      "3. Artificial Neural Networks classifier\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.40      0.47       108\n",
      "     neutral       0.17      0.06      0.09        67\n",
      "    positive       0.90      0.97      0.93      1085\n",
      "\n",
      "    accuracy                           0.87      1260\n",
      "   macro avg       0.55      0.47      0.50      1260\n",
      "weighted avg       0.84      0.87      0.85      1260\n",
      "\n",
      "\n",
      "***attributes:  100\n",
      "1. Decision Tree classifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.18      0.18      0.18       108\n",
      "     neutral       0.12      0.10      0.11        67\n",
      "    positive       0.88      0.89      0.89      1085\n",
      "\n",
      "    accuracy                           0.79      1260\n",
      "   macro avg       0.39      0.39      0.39      1260\n",
      "weighted avg       0.78      0.79      0.78      1260\n",
      "\n",
      "\n",
      "2. Multinomial Naive Bayes classifier\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.51      0.18      0.26       108\n",
      "     neutral       0.04      0.01      0.02        67\n",
      "    positive       0.88      0.98      0.93      1085\n",
      "\n",
      "    accuracy                           0.86      1260\n",
      "   macro avg       0.48      0.39      0.40      1260\n",
      "weighted avg       0.81      0.86      0.82      1260\n",
      "\n",
      "\n",
      "3. Artificial Neural Networks classifier\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.26      0.35       108\n",
      "     neutral       0.40      0.09      0.15        67\n",
      "    positive       0.89      0.97      0.93      1085\n",
      "\n",
      "    accuracy                           0.87      1260\n",
      "   macro avg       0.60      0.44      0.47      1260\n",
      "weighted avg       0.83      0.87      0.84      1260\n",
      "\n",
      "\n",
      "***attributes:  10\n",
      "1. Decision Tree classifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.19      0.17      0.18       108\n",
      "     neutral       0.11      0.09      0.10        67\n",
      "    positive       0.88      0.90      0.89      1085\n",
      "\n",
      "    accuracy                           0.79      1260\n",
      "   macro avg       0.39      0.38      0.39      1260\n",
      "weighted avg       0.78      0.79      0.79      1260\n",
      "\n",
      "\n",
      "2. Multinomial Naive Bayes classifier\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.02      0.04       108\n",
      "     neutral       0.00      0.00      0.00        67\n",
      "    positive       0.86      1.00      0.93      1085\n",
      "\n",
      "    accuracy                           0.86      1260\n",
      "   macro avg       0.62      0.34      0.32      1260\n",
      "weighted avg       0.83      0.86      0.80      1260\n",
      "\n",
      "\n",
      "3. Artificial Neural Networks classifier\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.37      0.06      0.11       108\n",
      "     neutral       0.08      0.01      0.02        67\n",
      "    positive       0.87      0.99      0.92      1085\n",
      "\n",
      "    accuracy                           0.85      1260\n",
      "   macro avg       0.44      0.35      0.35      1260\n",
      "weighted avg       0.79      0.85      0.81      1260\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "y = data['rating']\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x.split(), max_features=1000)\n",
    "\n",
    "x_dense = vectorizer.fit_transform(data['verified_reviews'])\n",
    "x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "print('***attributes: ALL')\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_sparse, y, test_size=0.4, shuffle = False)\n",
    "\n",
    "print('1. Decision Tree classifier')\n",
    "classifier = tree.DecisionTreeClassifier()\n",
    "model = classifier.fit(x_train, y_train)\n",
    "y_test_predicted = model.predict(x_test)\n",
    "print(classification_report(y_test, y_test_predicted))\n",
    "print()\n",
    "\n",
    "print('2. Multinomial Naive Bayes classifier')\n",
    "print()\n",
    "classifier = MultinomialNB()\n",
    "model = classifier.fit(x_train, y_train)\n",
    "y_test_predicted = model.predict(x_test)\n",
    "print(classification_report(y_test, y_test_predicted))\n",
    "print()\n",
    "\n",
    "print('3. Artificial Neural Networks classifier')\n",
    "print()\n",
    "classifier = MLPClassifier(hidden_layer_sizes=200)\n",
    "model = classifier.fit(x_train, y_train)\n",
    "y_test_predicted = model.predict(x_test)\n",
    "print(classification_report(y_test, y_test_predicted))\n",
    "print()\n",
    "\n",
    "\n",
    "for att in [1000, 100, 10]:\n",
    "    print('***attributes: ', att)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(limit_attributes(data, att), y, test_size=0.4, shuffle = False)\n",
    "    \n",
    "    \n",
    "    print('1. Decision Tree classifier')\n",
    "    \n",
    "    # create a decision tree classifier\n",
    "    classifier = tree.DecisionTreeClassifier()\n",
    "    \n",
    "    # train the model \n",
    "    model = classifier.fit(x_train, y_train)\n",
    "    \n",
    "    y_test_predicted = model.predict(x_test)\n",
    "    \n",
    "    # print different evaluation metrics \n",
    "    print(classification_report(y_test, y_test_predicted))\n",
    "    print()\n",
    "    \n",
    "    print('2. Multinomial Naive Bayes classifier')\n",
    "    print()\n",
    "    \n",
    "    # create a decision tree classifier\n",
    "    classifier = MultinomialNB()\n",
    "    \n",
    "    # train the model \n",
    "    model = classifier.fit(x_train, y_train)\n",
    "    \n",
    "    y_test_predicted = model.predict(x_test)\n",
    "    \n",
    "    # print different evaluation metrics \n",
    "    print(classification_report(y_test, y_test_predicted))\n",
    "    print()\n",
    "    \n",
    "    print('3. Artificial Neural Networks classifier')\n",
    "    print()\n",
    "    \n",
    "    # create a decision tree classifier\n",
    "    classifier = MLPClassifier(hidden_layer_sizes=200)\n",
    "    \n",
    "    # train the model \n",
    "    model = classifier.fit(x_train, y_train)\n",
    "    \n",
    "    y_test_predicted = model.predict(x_test)\n",
    "    \n",
    "    # print different evaluation metrics \n",
    "    print(classification_report(y_test, y_test_predicted))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.3\n",
    "\n",
    "Evaluate the effect each scenario has on the performance for the three models based on the performance metrics. Explain the differences in the performance. Write your answer in the cell below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For decision tree model, more attributes is helpful for the overall accuracy improvement. For positive class, the impovement didn't change very much. Change in negative class and neutral class is more obvious than in positive class.\n",
    "\n",
    "  For MNB model, more attributes is helpful for the overall accuracy improvement. No influence on positive class, big impovment on other 2 classes. \n",
    "  \n",
    "  For ANN modle, more attributes is helpful for the overall accuracy improvement. Slightly increase accuracy for positive class, similar accuracy for negative and neutral class (expect attributes=5).\n",
    "  \n",
    "  But the accuracy for token number = 1000 and all tokens is similar for all three models, so there's no need to train all tokens, 1000 is enought in this model. More attributes is just waste of time and machine memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.4\n",
    "\n",
    "Remove all samples with neutral sentiment from the dataset. Train and test all three models on this new dataset, computing the preformance measures as well. Assume that the first 60% of samples constitute the traing set while the remaining 40% of samples constitute the test set. All tokens in the vocabulary should be used as attributes.\n",
    "\n",
    "Place your code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Decision Tree classifier\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.39      0.20      0.27       109\n",
      "    positive       0.92      0.97      0.95      1091\n",
      "\n",
      "    accuracy                           0.90      1200\n",
      "   macro avg       0.65      0.58      0.61      1200\n",
      "weighted avg       0.88      0.90      0.88      1200\n",
      "\n",
      "\n",
      "2. Multinomial Naive Bayes classifier\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.34      0.12      0.18       109\n",
      "    positive       0.92      0.98      0.95      1091\n",
      "\n",
      "    accuracy                           0.90      1200\n",
      "   macro avg       0.63      0.55      0.56      1200\n",
      "weighted avg       0.87      0.90      0.88      1200\n",
      "\n",
      "\n",
      "3. Artificial Neural Networks classifier\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.28      0.40       109\n",
      "    positive       0.93      0.99      0.96      1091\n",
      "\n",
      "    accuracy                           0.92      1200\n",
      "   macro avg       0.82      0.63      0.68      1200\n",
      "weighted avg       0.91      0.92      0.91      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "dropneu = data.copy()\n",
    "dropneu = dropneu.drop(dropneu[dropneu.rating == 'neutral'].index)\n",
    "dropneu = dropneu.dropna().reset_index(drop=True)\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x.split())\n",
    "\n",
    "x_dense = vectorizer.fit_transform(dropneu['verified_reviews'])\n",
    "x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "y = dropneu['rating']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_sparse, y, test_size=0.4, shuffle = False)\n",
    "    \n",
    "    \n",
    "print('1. Decision Tree classifier')\n",
    "print()\n",
    "\n",
    "# create a decision tree classifier\n",
    "classifier = tree.DecisionTreeClassifier()\n",
    "\n",
    "# train the model \n",
    "model = classifier.fit(x_train, y_train)\n",
    "\n",
    "y_test_predicted = model.predict(x_test)\n",
    "\n",
    "# print different evaluation metrics \n",
    "print(classification_report(y_test, y_test_predicted))\n",
    "print()\n",
    "\n",
    "print('2. Multinomial Naive Bayes classifier')\n",
    "print()\n",
    "\n",
    "# create a decision tree classifier\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# train the model \n",
    "model = classifier.fit(x_train, y_train)\n",
    "\n",
    "y_test_predicted = model.predict(x_test)\n",
    "\n",
    "# print different evaluation metrics \n",
    "print(classification_report(y_test, y_test_predicted))\n",
    "print()\n",
    "\n",
    "print('3. Artificial Neural Networks classifier')\n",
    "print()\n",
    "\n",
    "# create a decision tree classifier\n",
    "classifier = MLPClassifier(hidden_layer_sizes=200)\n",
    "\n",
    "# train the model \n",
    "model = classifier.fit(x_train, y_train)\n",
    "\n",
    "y_test_predicted = model.predict(x_test)\n",
    "\n",
    "# print different evaluation metrics \n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.5\n",
    "Compare these results to the results obtained in scenario #1 (part 4). Is there any difference in the metrics for either of the classes (i.e. consider positive and negative classes individually)?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Drop neutral class make a big promotion for accuracy of negative class, didn't make big differernce to positive class.It is beacuse the amount of data in positive class is huge compare to neutral class. \n",
    "But the amount of data in negative class is similar to neutral class, therefore a big change of accuracy for negative class.\n",
    "But the overall accuracy is increased, so it is a good decision to drop data in neutral class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 [8 marks]\n",
    "\n",
    "In this part you need to develop your own method (pipeline) for sentiment analysis. You can create new code cells (to place your code) and Raw NBConvert cells (to descibe the steps you are taking to develop your model, or make a comment).\n",
    "\n",
    "You can use either Decision Tree, Multinomial Naive Bayes (MNB), or Artificial Neural Network (ANN) classifiers. If you use parameters for your classifier, the you need to justify the values that you chose for them in writing or in coding & writing (if it is based on evaluation resuts) - similar to Part 2.\n",
    "\n",
    "Since this part of the assignment carries the highest mark, the explanation of your method and its evaluation should be descibed in detail.\n",
    "\n",
    "Optionally: you can also make plots to visualize your findings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First step is preprocess data.\n",
    "\n",
    "   For machine learning, preprocessing data is very important, not only helpful in imporving accuracy of prediction, but also can help reshaping data to make any math calculation easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from tsv file\n",
    "origin = pd.read_csv('amazon_alexa.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, there are 5 classes in 'rating' column, to increase the prediction acccuracy, merge similar classes into one is a good method. So data in 'rating' changed to three class 'negative', 'neutral' and 'positive'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change 5 rating levels into 3 classes\n",
    "origin.loc[origin['rating'] == 1, 'rating'] = 'negative'\n",
    "origin.loc[origin['rating'] == 2, 'rating'] = 'negative'\n",
    "origin.loc[origin['rating'] == 3, 'rating'] = 'neutral'\n",
    "origin.loc[origin['rating'] == 4, 'rating'] = 'positive'\n",
    "origin.loc[origin['rating'] == 5, 'rating'] = 'positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the orignal data, I found that the 'feedback' column is '0' when the rating is '1' or '2', which is 'negative' review, and for neutral and positive classes, 'feedback' is always '1', which can be used to separate and identify reviews in different class. So I merged 'verified_reviews' column and 'feedback' column to a new column: 'newcolumn' at first. After I run the code, the accuracy increased from 91% t0 96%, which is a big improvment, but then I noticed that the current feedback is depend on the rating that customer made. When dealing with reviews in real life outside the test set, there will be no rating and no feedback availiable to help machine learning. So I decide NOT to use 'feedback' column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre4 = origin.copy()\n",
    "\n",
    "# merge 'verified_reviews' column and 'feedback' column to a new column (not choosen)\n",
    "pre4['newcolumn'] = pre4['verified_reviews'] +' '+ pre4['feedback'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is to remove URLS, symbols and stopwords, and make all words into lower case. Because including those is not meaningful for machine identifing the rating of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove URLS and symbols, and make all words into lower case\n",
    "pre4['verified_reviews'] = pre4['verified_reviews'].map(lambda pre: re.sub(r'https?\\S+', '', pre).lower())\n",
    "pre4['verified_reviews'] = pre4['verified_reviews'].map(lambda pre: re.sub(r'[^A-Za-z\\d\\'\\#\\@\\$\\s*]', '', pre).lower())\n",
    "\n",
    "# remove stopwords\n",
    "stopword = stopwords.words('english')\n",
    "listnew = []\n",
    "for i in pre4.index:\n",
    "    verifiedreviews = pre4.verified_reviews[i].split()\n",
    "    listnew.append(' '.join([item for item in verifiedreviews if item not in stopword]))\n",
    "\n",
    "pre4['verified_reviews'] = listnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the data in neutral class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop data in neutral class (not choosen)\n",
    "dropneu = pre4.copy()\n",
    "dropneu = dropneu.drop(dropneu[dropneu.rating == 'neutral'].index)\n",
    "dropneu = dropneu.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "origin['rating'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Advantage for drop data in neutral class:\n",
    "(1) From the plot above, data in neutral class is in very small percentage,\n",
    "(2) neutral class is not important as reviews in positive class or negative class, \n",
    "(3) it helps increasing accuracy of prediction\n",
    "I still decide not drop neutral class because there are lots of other ways we can to with data to improving prediction accuracy, drop a class from three is not a good idea beacuse if we use this model for other types of predict rating, the neutral evaluation might be important.\n",
    "So I decided not to drop any data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final step for preprocessing data is to convert sentances in 'verified_reciews' into sparse matrix, because sparse matrix form is more convenient for machine learning, because we need to use those data for machine learning, data in sparce matrix form makes mechine learning doing basic mathematical operation and other computation easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# into sparse matrix\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x.split(), max_features=1000)\n",
    "x_dense = vectorizer.fit_transform(pre4['verified_reviews'])\n",
    "x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Next is choose classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Decision Tree, Multinomial Naive Bayes (MNB), or Artificial Neural Network (ANN) classifiers, I decided to use ANN classifier. Because from same conditions, ANN got highest accuracy in almost situations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that is to adjust parameter inside classifier. First thing came into my mind is to change the number of hidden neurons, "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hidden neurons = 256 accuracy:[0.90, 0.89, 0.90, 0.89, 0.91, 0.90, 0.90, 0.90, 0.90, 0.89] average accuracy:0.898\n",
    "hidden neurons = 668 accuracy:[0.90, 0.90, 0.89, 0.92, 0.90, 0.90, 0.91, 0.91, 0.90, 0.90] average accuracy:0.903\n",
    "hidden neurons = 900 accuracy:[0.90, 0.90, 0.91, 0.89, 0.90, 0.89, 0.90, 0.89, 0.90, 0.90] average accuracy:0.898"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when the number goes higher, the time spend on training data goes slower. When number of hidden neurons is 668, the average accuracy and the time spend is relatively good."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "solver=lbfgs [0.88, 0.90, 0.90, 0.88, 0.90, 0.89, 0.90, 0.90, 0.90, 0.90] average accuracy:0.895\n",
    "solver=adam  [0.89, 0.89, 0.91, 0.90, 0.92, 0.89, 0.89, 0.91, 0.92, 0.90] average accuracy:0.902\n",
    "solver=sgd   [0.87, 0.88, 0.86, 0.87, 0.88, 0.86, 0.88, 0.88, 0.87, 0.88] average accuracy:0.873"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time spend by lbfgs solver is the least, but adam gives the highest accuracy."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "activation=identity [0.89, 0.90, 0.88, 0.89, 0.89, 0.88, 0.89, 0.89, 0.90, 0.90] average accuracy: 0.891\n",
    "activation=logistic [0.90, 0.89, 0.89, 0.89, 0.89, 0.90, 0.88, 0.89, 0.89, 0.90] average accuracy: 0.892\n",
    "activation=tanh     [0.88, 0.89, 0.89, 0.90, 0.88, 0.89, 0.88, 0.89, 0.88, 0.87] average accuracy: 0.885\n",
    "activation=relu     [0.91, 0.88, 0.90, 0.90, 0.91, 0.90, 0.89, 0.89, 0.90, 0.92] average accuracy: 0.900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ReLU as a activate function, it shows the highest overall prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the effect of different parameter value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_data = ['256', '668', '900', 'lbfgs', 'sgd', 'identity', 'logistic', 'tanh']\n",
    "y_data = [0.898, 0.903, 0.898, 0.895, 0.873, 0.891, 0.892, 0.885]\n",
    "plt.plot(x_data,y_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pre4['rating']\n",
    "# separate data set into training set and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_sparse, y, test_size=0.4)\n",
    "\n",
    "# use ANN classifier\n",
    "\n",
    "# size256 = MLPClassifier(hidden_layer_sizes=256, solver='adam', activation='relu')\n",
    "# size900 = MLPClassifier(hidden_layer_sizes=900, solver='adam', activation='relu')\n",
    "\n",
    "# solverAdam = MLPClassifier(hidden_layer_sizes=668, solver='adam', activation='relu')\n",
    "# solverSgd = MLPClassifier(hidden_layer_sizes=668, solver='sgd', activation='relu')\n",
    "\n",
    "# activationIdentity = MLPClassifier(hidden_layer_sizes=668, solver='adam', activation='identity')\n",
    "# activationLogistic = MLPClassifier(hidden_layer_sizes=668, solver='adam', activation='logistic')\n",
    "# activationTanh = MLPClassifier(hidden_layer_sizes=668, solver='adam', activation='tanh')\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=668, solver='adam', activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model \n",
    "model = classifier.fit(x_train, y_train)\n",
    "y_test_predicted = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print different evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.63      0.43      0.51       107\n",
      "     neutral       0.68      0.33      0.45        63\n",
      "    positive       0.92      0.98      0.95      1090\n",
      "\n",
      "    accuracy                           0.90      1260\n",
      "   macro avg       0.74      0.58      0.64      1260\n",
      "weighted avg       0.88      0.90      0.89      1260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print different evaluation metrics \n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall, I merge 5 rating levels into 3 classes; remove URLS, symbols, stopwords and make all words into lower case; convert sentances in 'verified_reciews' into sparse matrix; use ANN classifier with number of hidden neurons = 668. And the final accuracy is arround 91%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
